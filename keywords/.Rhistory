}
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
require(gbm)
boost.boston=gbm(medv~., data=Boston[train,], distribution = "gaussian", n.trees=10000
, shrinkage=0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i="lstat")
plot(boost.boston, i="rm")
n.trees=seq(from=100, to=10000, by=100)
predmat= predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
dim(predmat)
predmat= predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
predmat = predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
head(Boston)
n.trees=seq(from=100, to=10000, by=100)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
plot(n.trees, berr, pch=19, ylab="Mean Squared Error", xlab="# Trees", main= "Boosting Test Error")
abline(h=min(test.err),col="red")
matrix(nrow= 4, ncol= 8)
?apply
t?
?t
require(devtools )
install_github(&quot;andreacirilloac/updateR&quot;)
install_github(&quot;andreacirilloac/updateR&quot;)
version
library(RPostgreSQL)
library(stringdist)
library(plyr)
library(stringr)
library(tm)
library(stringi)
# ######### CONNECTION TO DB ###############
# loads the PostgreSQL driver
drv <- dbDriver("PostgreSQL")
con <- dbConnect(
drv, dbname = "smart",
host = "50.16.139.89",
port = 5432,
user = "dmkm",
password = "dmkm1234"
)
# Query to retrieve combinations of Tweets - Kewyords
query_kw <- " select t.idd::varchar(100), t.text, k.id, k.keyword
from twitter.tweets t,
twitter.keywords k
limit 200;"
# Retreives the table from the database
df <- dbGetQuery(con, query_kw)
query_kw <- " select t.idd::varchar(100), t.text, k.id, k.keyword
from twitter.tweets t,
twitter.keywords k
limit 2000;"
# Retreives the table from the database
df <- dbGetQuery(con, query_kw)
query_kw <- " select t.idd::varchar(100), t.text, k.id, k.keyword
from twitter.tweets t,
twitter.keywords k
limit 20000;"
# Retreives the table from the database
df <- dbGetQuery(con, query_kw)
query_kw <- " select t.idd::varchar(100), t.text, k.id, k.keyword
from twitter.tweets t,
twitter.keywords k
limit 200000;"
query_kw <- " select t.idd::varchar(100), t.text, k.id, k.keyword
from twitter.tweets t,
twitter.keywords k
limit 400000;"
# Retreives the table from the database
df <- dbGetQuery(con, query_kw)
tail(df)
names(df)
jaccard_distance<- function(data,var1,var2){
x<-stringdist(data[,var1], data[,var2], method= "jaccard")
x[which(x==Inf)] <- 1
as.numeric(x)
}
cosine_distance<- function(data,var1,var2){
x<-stringdist(data[,var1], data[,var2], method= "cosine")
x[which(x==Inf)] <- 1
as.numeric(x)
}
clean_text<- function(text)
{
text = laply(text, function(text)
{
# clean up sentences with R's regex-driven global substitute, gsub():
text = gsub("[\']", " ", text)
text = gsub('[[:punct:]]', '', text)
text = gsub('[[:cntrl:]]', '', text)
text = gsub('\\d+', '', text)
text = stri_trans_general(text ,"Latin-ASCII")
text = gsub('http\\S+\\s*', '', text)
#Clean emojis
text <- sapply(text, function(row) iconv(row, "latin1", "ASCII", sub=" "))
names(text) <- NULL
# replace al \n
text <- gsub("[\n]", " ", text)
# convert to lower case:
text = tolower(text)
return(text)
})
###Remove Stopwords
stopWords<-stopwords(kind="french")
'%nin%' <- Negate('%in%')
text <-lapply(text, function(x) {
t <- unlist(strsplit(x, " "))
t[t %nin% stopWords]
})
text <- sapply(text, paste, collapse=" ")
return(text)
}
# Counts the amount of keywords presented in the tweet
count_substring = function(text,keywords) {
subcount = function(text,keywords){
kw = unlist(strsplit(keywords, ","))
c=0
for(i in length(kw)){
if(grepl(kw[i],text)){
c=c+1
} else {c}
}
return(c/length(kw))
}
mapply(subcount, text, keywords, USE.NAMES = FALSE)
}
df$text_clean = clean_text(df$text)
head(df)
tweet_to_ip = df %>% filter(count > 0) %>% select(idd, id)
library(dplyr)
tweet_to_ip = df %>% filter(count > 0) %>% select(idd, id)
head(tweet_to_ip)
head(df)
df$count = count_substring(df$text_clean, df$keyword)
tweet_to_ip = df %>% filter(count > 0) %>% select(idd, id)
head(tweet_to_ip)
dbExistsTable(con, c("ip","interest_points"))
query_kw <- "select id, name
from ip.interest_points
where in_use = True
;"
names <- dbGetQuery(con, query_kw)
head(names)
class(names$id)
query_kw <- "select id::varchar(100), name
from ip.interest_points
where in_use = True
;"
names <- dbGetQuery(con, query_kw)
class(names$id)
unwanted_array = list(    'Š'='S', 'š'='s', 'Ž'='Z', 'ž'='z', 'À'='A', 'Á'='A', 'Â'='A', 'Ã'='A', 'Ä'='A', 'Å'='A', 'Æ'='A', 'Ç'='C', 'È'='E', 'É'='E',
'Ê'='E', 'Ë'='E', 'Ì'='I', 'Í'='I', 'Î'='I', 'Ï'='I', 'Ñ'='N', 'Ò'='O', 'Ó'='O', 'Ô'='O', 'Õ'='O', 'Ö'='O', 'Ø'='O', 'Ù'='U',
'Ú'='U', 'Û'='U', 'Ü'='U', 'Ý'='Y', 'Þ'='B', 'ß'='Ss', 'à'='a', 'á'='a', 'â'='a', 'ã'='a', 'ä'='a', 'å'='a', 'æ'='a', 'ç'='c',
'è'='e', 'é'='e', 'ê'='e', 'ë'='e', 'ì'='i', 'í'='i', 'î'='i', 'ï'='i', 'ð'='o', 'ñ'='n', 'ò'='o', 'ó'='o', 'ô'='o', 'õ'='o',
'ö'='o', 'ø'='o', 'ù'='u', 'ú'='u', 'û'='u', 'ý'='y', 'ý'='y', 'þ'='b', 'ÿ'='y' )
stopwordsFr<-stopwords(kind="french")
stopwordsEn<-stopwords(kind="en")
stopWords<- c(stopwordsEn,stopwordsFr,"lyon")
names.lower<- as.data.frame(lapply(names, tolower))
names.lower$name<- as.character(names.lower$name)
head(names.lower)
#Clean
names.lower$nom_noacc<-names.lower$name
names.lower$keyword<-names.lower$name
for(i in 1:nrow(names.lower)){
#Get rid of punctuations
names.lower[i,3:4]<-gsub('[[:punct:]]', " ", names.lower[i,3])
#Get rid of symbols
names.lower[i,3:4]<-gsub('[[:cntrl:]]', " ", names.lower[i,3])
names.lower[i,3:4]<-gsub(" - ", " ", names.lower[i,3])
#Get rid of the accents
names.lower[i,3:4]<-chartr(paste(names(unwanted_array), collapse=''),
paste(unwanted_array, collapse=''),
names.lower[i,2])
}
######## Remove StopWords #####################
'%nin%' <- Negate('%in%')
X<-lapply(names.lower[,4], function(x) {
t <- unlist(strsplit(x, " "))
t[t %nin% stopWords]
})
#Remove extra spaces
names.lower[,4]<- sapply(X, paste, collapse=",")
keywords = names.lower[,c(1,4)]
head(keywords)
dbRemoveTable(con,c("twitter","keywords") )
dbWriteTable(con, c("twitter","keywords"), keywords)
query_kw <- " select t.idd::varchar(100), t.text, k.id, k.keyword
from twitter.tweets t,
twitter.keywords k
limit 400000;"
df <- dbGetQuery(con, query_kw)
# ######### CONNECTION TO DB ###############
############# FUNCTIONS   ##################
clean_text<- function(text)
{
text = laply(text, function(text)
{
# clean up sentences with R's regex-driven global substitute, gsub():
text = gsub("[\']", " ", text)
text = gsub('[[:punct:]]', '', text)
text = gsub('[[:cntrl:]]', '', text)
text = gsub('\\d+', '', text)
text = stri_trans_general(text ,"Latin-ASCII")
text = gsub('http\\S+\\s*', '', text)
#Clean emojis
text <- sapply(text, function(row) iconv(row, "latin1", "ASCII", sub=" "))
names(text) <- NULL
# replace al \n
text <- gsub("[\n]", " ", text)
# convert to lower case:
text = tolower(text)
return(text)
})
###Remove Stopwords
stopWords<-stopwords(kind="french")
'%nin%' <- Negate('%in%')
text <-lapply(text, function(x) {
t <- unlist(strsplit(x, " "))
t[t %nin% stopWords]
})
text <- sapply(text, paste, collapse=" ")
return(text)
}
# Counts the amount of keywords presented in the tweet
count_substring = function(text,keywords) {
subcount = function(text,keywords){
kw = unlist(strsplit(keywords, ","))
c=0
for(i in length(kw)){
if(grepl(kw[i],text)){
c=c+1
} else {c}
}
return(c/length(kw))
}
mapply(subcount, text, keywords, USE.NAMES = FALSE)
}
############# FUNCTIONS   ##################
############################################
############# MODEL   ##################
df$text_clean = clean_text(df$text)
df$count = count_substring(df$text_clean, df$keyword)
tweet_to_ip = df %>% filter(count > 0) %>% select(idd, id)
head(tweet_to_ip)
?dbWriteTable
dbWriteTable(con, c("twitter","tweet_to_ip"), tweet_to_ip, overwrite=FALSE, append=TRUE) #Meanwhile we find out how to update
head(tweet_to_ip)
names(tweet_to_ip) = c("twitter_id","ip_id")
tweet_to_ip<- tweet_to_ip[,c(2,1)]
head(tweet_to_ip)
dbWriteTable(con, c("twitter","tweet_to_ip"), tweet_to_ip, overwrite=FALSE, append=TRUE) #Meanwhile we find out how to update
dbWriteTable(con, c("twitter","tweet_to_ip"), tweet_to_ip, append=TRUE) #Meanwhile we find out how to update
# ######### CONNECTION TO DB ###############
# loads the PostgreSQL driver
drv <- dbDriver("PostgreSQL")
con <- dbConnect(
drv, dbname = "smart",
host = "50.16.139.89",
port = 5432,
user = "dmkm",
password = "dmkm1234"
)
dbWriteTable(con, c("twitter","tweet_to_ip"), tweet_to_ip, append=TRUE) #Meanwhile we find out how to update
dbWriteTable(con, c("twitter","tweet_to_ip"), tweet_to_ip, overwrite=TRUE, append=TRUE) #Meanwhile we find out how to update
dbWriteTable(con, c("twitter","tweet_to_ip"), tweet_to_ip, overwrite=TRUE) #Meanwhile we find out how to update
names(tweet_to_ip)
update <- function(i, con, tweet_to_ip) {
txt <- paste("INSERT tweets.tweets SET ip_id=",tweet_to_ip$ip_id[i],"WHERE twitter_id=",tweet_to_ip$twitter_id[i],"::bigint;")
dbGetQuery(con, txt)
}
for (i in 1:length(tweet_to_ip$ip_id)){
update(i, con, tweet_to_ip)
}
dbDropTable(con, c("twitter","ip_to_tweet"), tweet_to_ip)
dbRemoveTable(con, c("twitter","ip_to_tweet"), tweet_to_ip)
dbWriteTable(con, c("twitter","ip_to_tweet"), tweet_to_ip)
install.packages("jsonlite")
library(jsonlite)
login <- fromJSON("../login.json", flatten=TRUE)
setwd("/Users/saulgarcia/Desktop/Github/MOOCs/MIT Analyticals Edge/Week6")
movies = read.table("movieslens.txt", header=FALSE, sep="|", quote="\"")
list.files()
movies = read.table("movielens.txt", header=FALSE, sep="|", quote="\"")
str(movies)
colnames(movies) = c("ID","Title","ReleaseDate","VideoReleaseDate","IMDB","Unknown","Action","Adventure",
"Animation","Childrens","Comedy","Crime", "Documentary","Drama","Fantasy","FilmNoir",
"Horror","Musical","Mystery","Romance","SciFi","Thriller","War","Western")
str(movies)
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
movies = unique(movies)
str(movies)
table(movies$Comedy)
table(movies$Western)
table(movies$Romance)
table(movies$Drama)
716 + 244
table(movies$Romance & movies$Drama)
distances = dist(movies[2:20], method="euclidean")
clusterMovies = hclust(distances, method = "ward.D")
plot(clusterMovies)
distance
distances
clusterGroups = cutree(clusterMovies, k=10)
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
subset(movies,Title=="Men in Black (1997)")
clusterGroups[257]
cluster2= subset(movies, clusterGroups==2)
cluster2$Title[1:10]
cluster5= subset(movies, clusterGroups==5)
cluster5$Title[1:10]
spl = split(movies[2:20],clusterGroups)
head(spl)
lapply(spl, colMeans)
subset(movies, clusterGroups ==2)
subset(movies, clusterGroups == 1)
clusterG = cutree(clusterMovies, k=2)
tapply(movies$Action, clusterGroups, mean)
spl2 = split(movies[2:20], clusterG)
lapply(spl2,colMeans)
dbDisconnect(con)
getwd()
setwd("/Users/saulgarcia/Desktop/SmartCity/keywords")
setwd("/Users/saulgarcia/Desktop/Github/SmartCity/keywords")
library(jsonlite)
login <- fromJSON("../login.json", flatten=TRUE)
con <- dbConnect(
drv, dbname = login$dbname,
host = login$host,
port = login$port,
user = login$user,
password = login$password
)
dbDisconnect(con)
login <- fromJSON("../login.json", flatten=TRUE)
con <- dbConnect(
drv, dbname = login$dbname,
host = login$host,
port = login$port,
user = login$user,
password = login$password
)
dbExistsTable(con,tweets)
dbExistsTable(con,c("twitter","tweet_to_ip"))
head(tweet_to_ip)
insert <- function(i, con, tweet_to_ip) {
txt <- paste("INSERT into twitter.tweet_to_ip values (",tweet_to_ip$ip_id[i],", ",tweet_to_ip$twitter_id[i],"::bigint);")
dbGetQuery(con, txt)
}
for (i in 1:length(tweet_to_ip$ip_id)){
insert(i, con, tweet_to_ip)
}
test1 = dbReadTable(con, c("twitter","tweet_to_ip"))
head(test1)
query_kw <- "
select
t.idd::varchar(100),
t.text,
k.id,
k.keyword
from
twitter.tweets t,
twitter.keywords k
where
t.idd not in (
select distinct twitter_id
from twitter.tweet_to_ip
)
and t.idd in (select distinct twitter_id
from twitter.tweets
limit 2000
);"
df <- dbGetQuery(con, query_kw)
query_kw <- "
select
t.idd::varchar(100),
t.text,
k.id,
k.keyword
from
twitter.tweets t,
twitter.keywords k
where
t.idd not in (
select distinct twitter_id
from twitter.tweet_to_ip
)
and t.idd in (select distinct idd
from twitter.tweets
limit 2000
);"
df <- dbGetQuery(con, query_kw)
clean_text<- function(text)
{
text = laply(text, function(text)
{
# clean up sentences with R's regex-driven global substitute, gsub():
text = gsub("[\']", " ", text)
text = gsub('[[:punct:]]', '', text)
text = gsub('[[:cntrl:]]', '', text)
text = gsub('\\d+', '', text)
text = stri_trans_general(text ,"Latin-ASCII")
text = gsub('http\\S+\\s*', '', text)
#Clean emojis
text <- sapply(text, function(row) iconv(row, "latin1", "ASCII", sub=" "))
names(text) <- NULL
# replace al \n
text <- gsub("[\n]", " ", text)
# convert to lower case:
text = tolower(text)
return(text)
})
###Remove Stopwords
stopWords<-stopwords(kind="french")
'%nin%' <- Negate('%in%')
text <-lapply(text, function(x) {
t <- unlist(strsplit(x, " "))
t[t %nin% stopWords]
})
text <- sapply(text, paste, collapse=" ")
return(text)
}
# Counts the amount of keywords presented in the tweet
count_substring = function(text,keywords) {
subcount = function(text,keywords){
kw = unlist(strsplit(keywords, ","))
c=0
for(i in length(kw)){
if(grepl(kw[i],text)){
c=c+1
} else {c}
}
return(c/length(kw))
}
mapply(subcount, text, keywords, USE.NAMES = FALSE)
}
df$text_clean = clean_text(df$text)
df$count = count_substring(df$text_clean, df$keyword)
tweet_to_ip = df %>% filter(count > 0) %>% select(idd, id)
names(tweet_to_ip) = c("twitter_id","ip_id")
tweet_to_ip<- tweet_to_ip[,c(2,1)]
insert <- function(i, con, tweet_to_ip) {
txt <- paste("INSERT into twitter.tweet_to_ip values (",tweet_to_ip$ip_id[i],", ",tweet_to_ip$twitter_id[i],"::bigint);")
dbGetQuery(con, txt)
}
for (i in 1:length(tweet_to_ip$ip_id)){
insert(i, con, tweet_to_ip)
}
query_kw <- "
select
id::varchar(100), name
from
ip.interest_points p
where
in_use = True
and t.id not in (
select distinct id
from twitter.keywords
)
;"
names <- dbGetQuery(con, query_kw)
query_kw <- "
select
id::varchar(100), name
from
ip.interest_points p
where
in_use = True
and p.id not in (
select distinct id
from twitter.keywords
)
;"
names <- dbGetQuery(con, query_kw)
query_kw <- "
select
id::varchar(100), name
from
ip.interest_points
where
in_use = True
and id not in (
select distinct id
from twitter.keywords
)
;"
names <- dbGetQuery(con, query_kw)
query_kw <- "
select
t.id::varchar(100), t.name
from
ip.interest_points t
where
t.in_use = True
and t.id not in (
select distinct id
from twitter.keywords
)
;"
names <- dbGetQuery(con, query_kw)
